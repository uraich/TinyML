{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c7b0c7-d549-489b-8c53-65f734b738f1",
   "metadata": {},
   "source": [
    "# Evaluate the mnist model using 10 png test files\n",
    "Import the needed modules. We use the tflite-micro runtime interpreter for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdf205b-e9aa-492f-b879-25e940c71010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828cbd7-dfbc-4fae-9c43-f3b49eebe545",
   "metadata": {},
   "source": [
    "Tensorflow as well as the tensorflow lite interpreter are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a055e6-75d4-4b59-8d19-445aac633321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 10:32:04.076834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-22 10:32:04.076883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-22 10:32:04.078510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-22 10:32:04.087171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-22 10:32:05.164424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tflite_runtime.interpreter as tflite\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3970cae-2d49-4221-bd62-e4b0a949ed76",
   "metadata": {},
   "source": [
    "An tflite interpreter is created and allocate_tensors is called to prepare it for use  \n",
    "It is provided with the trained model converted to tflite format and quantized in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18881754-a872-4680-b03b-a0922ef0cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tflite.Interpreter(model_path=\"models/number_model_quant.tflite\")\n",
    "interpreter.allocate_tensors() # Needed before execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f10aa-debd-42df-bb51-aa9d59be276d",
   "metadata": {},
   "source": [
    "The samples used to verify the model are available as png image files  \n",
    "In order to present them to the model we need one image represented by a matrix of 28x28 pixels \n",
    "read_img reads the image from the png file and provides the conversion. This function can be used for any of the sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d707d5f-0ab9-4bb7-81e9-b413dbbcbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_path):\n",
    "  \"\"\"Read MNIST image\n",
    "\n",
    "  Args:\n",
    "      img_path (str): path to a MNIST image\n",
    "\n",
    "  Returns:\n",
    "      np.array : image in the correct np.array format\n",
    "  \"\"\"\n",
    "  image = Image.open(img_path)\n",
    "  data = np.asarray(image, dtype=np.float32)\n",
    "  if data.shape not in [(28, 28), (28, 28, 1)]:\n",
    "    raise ValueError(\n",
    "        \"Invalid input image shape (MNIST image should have shape 28*28 or 28*28*1)\"\n",
    "    )\n",
    "  # Normalize the image if necessary\n",
    "  if data.max() > 1:\n",
    "    data = data / 255.0\n",
    "  # Model inference requires batch size one\n",
    "  data = data.reshape((1, 28, 28))\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbba357-9084-40c7-9020-495df400271e",
   "metadata": {},
   "source": [
    "Let's try to plot the 10 image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6368f726-87af-43a4-93b7-218394751e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_array = [None]*10\n",
    "for img in range(10):\n",
    "    filename = \"samples/sample{:d}.png\".format(img)\n",
    "    pixel_array[img] = read_img(filename)\n",
    "\n",
    "plt.set_cmap('Greys')\n",
    "fig, axs = plt.subplots(2,5)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        axs[i,j].axis('off')\n",
    "        axs[i,j].imshow((pixel_array[5*i+j]).reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc57e8d-bfe7-456f-8900-5a0a7738aacf",
   "metadata": {},
   "source": [
    "get_input details tells us which format is needed at the input of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b1c4b-e45b-43fe-917a-defede6e1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "print(input_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf2ad-eb95-4ba7-a68d-0c287b54c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = input_details[\"dtype\"]\n",
    "print(\"data type: \",data_type)\n",
    "print(\"index: \",input_details[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af447794-3200-4c9d-ad0a-20595c810105",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_quantization_parameters = input_details[\"quantization_parameters\"]\n",
    "input_scale, input_zero_point = input_quantization_parameters[\"scales\"][0], input_quantization_parameters[\"zero_points\"][0]\n",
    "print(\"input scale: {:f}, input_zero_point: {:d}\".format(input_scale,input_zero_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d516e9e-0c0d-4354-9b32-17b2d8b130bf",
   "metadata": {},
   "source": [
    "get_output_details informs us about he output format of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db56e1b-a5a8-4af6-8560-8de803fd4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_details = interpreter.get_output_details()[0]\n",
    "print(output_details)\n",
    "print(\"\\nOutput data type: \",output_details[\"dtype\"])\n",
    "print(\"index: \",output_details[\"index\"])\n",
    "print(\"shape of output tensor: \",output_details['shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b7d70-0cdc-4047-bbe4-9dc7433ff749",
   "metadata": {},
   "source": [
    "Now we try the whole thing using a single sample file  \n",
    "First reading the file and converting it to the right format for input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e638f-1951-4f21-9ab9-27ef533c8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_img(\"samples/sample8.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab61efb-2794-4958-90f6-e33a685140dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min and max in pixel data: {:f}, {:f}\".format(data.min(),data.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3813971-29d8-4942-a903-e3fcc56fed14",
   "metadata": {},
   "source": [
    "The pixel values are represented as float values inn the range 0..1  \n",
    "They must be converted to int8 values -128 .. 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6962c8-ff45-4f75-ada0-0587092c14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data / input_scale + input_zero_point).astype(data_type)\n",
    "print(\"min and max in pixel data after quantization: {:f}, {:f}\",data.min(),data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f1514-a225-4ab0-9b25-3c0d79fff51e",
   "metadata": {},
   "source": [
    "We present these data to the input nodes of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a9a2d-4709-4a79-995a-f5be21e45f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details['index'],data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d8b58-7fe1-48a5-86ea-27363bfaef29",
   "metadata": {},
   "source": [
    "Invoking the interpreter corresponds to making a prediction for the data based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec0a57-b8d8-4ed0-9d39-ab862cf8d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details['index']).reshape(10,)\n",
    "print(\"shape of output_data :\",output_data.shape)\n",
    "print(output_data, type(output_data),output_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c33cfd-50c4-4fd9-b00e-9da19d9296ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = np.argmax(output_data)\n",
    "print(\"predicted digit: \",digit)\n",
    "print(output_data)\n",
    "probs = [None]*10\n",
    "print(\"probabilites: \")\n",
    "for i in range(10):\n",
    "    probs[i] = (output_data[i] + 128) / 255\n",
    "    print(\"{:6.4f}\".format(probs[i]), end=\" \")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32889f33-9353-43ee-8cfc-557e0f9bcc2c",
   "metadata": {},
   "source": [
    "To get the probabilities we have to dequantize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884afc12-6659-4a90-9621-e74c2eb3bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dequantize the result\n",
    "output_quantization_parameters = output_details[\"quantization_parameters\"]\n",
    "output_scale, output_zero_point = output_quantization_parameters[\"scales\"][0], output_quantization_parameters[\"zero_points\"][0]\n",
    "print(output_scale * (output_data.astype(\"float\") - output_zero_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39153d8-d3cd-43b7-80e2-6ce025228701",
   "metadata": {},
   "source": [
    "Now that we know how to invoke the interpreter for a single image and how to interpret the result, we can do this for all\n",
    "10 sample images and calculate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a6c8b-50e8-4f81-96b0-708e7d819e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [None] * 10\n",
    "probabilities = [None] * 10\n",
    "raw_output_data = [None] * 10\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "print(\"input data type: \", input_details[\"dtype\"])\n",
    "# get the input quantization parameters and quantize the pixel values\n",
    "input_quantization_parameters = input_details[\"quantization_parameters\"]\n",
    "input_scale, input_zero_point = input_quantization_parameters[\"scales\"][0], input_quantization_parameters[\"zero_points\"][0]\n",
    "print(\"input scale: {:f}, input_zero_point: {:d}\".format(input_scale,input_zero_point))\n",
    "output_quantization_parameters = output_details[\"quantization_parameters\"]\n",
    "output_scale, output_zero_point = output_quantization_parameters[\"scales\"][0], output_quantization_parameters[\"zero_points\"][0]\n",
    "print(\"output scale: {:f}, output_zero_point: {:d}\".format(output_scale,output_zero_point))\n",
    "\n",
    "for img in range(10):\n",
    "    # construct the filename\n",
    "    filename = \"samples/sample{:d}\".format(img) + \".png\"\n",
    "    images[img] = read_img(filename)\n",
    "    if img == 0:\n",
    "        print(\"data type of pixels: \",images[img].dtype)\n",
    "    # quantize the image data\n",
    "    images[img] = (images[img] / input_scale + input_zero_point).astype(data_type)\n",
    "    if img == 0:\n",
    "        print(\"Data type of pixels passed to the interpreter: \",images[img].dtype)\n",
    "    if img == 0:\n",
    "        print(\"min and max pixel values after quantization: {:d},{:d}\".format(images[img].min(),images[img].max()))\n",
    "    # set the input tensor\n",
    "    interpreter.set_tensor(input_details['index'],images[img])\n",
    "    # and invoke inference\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details['index']).reshape(10,)\n",
    "    #if img == 0:\n",
    "    print(output)\n",
    "    # dequantize to get the probabilities\n",
    "    probabilities[img] = output_scale * (output.astype(\"float\") - output_zero_point)\n",
    "    raw_output_data[img] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c852fa4-8e2f-48a8-ac9c-c640f864e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The confusion matrix:\")\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        print(\"{:8.6f}\".format(probabilities[i][j]), end=\" \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304837de-0674-4338-a2d5-46169753b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"Digits predicted: {:d}\".format(probabilities[i].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca4a65-b224-4d60-8cd3-0d2e763fb93c",
   "metadata": {},
   "source": [
    "Try to improve probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ecd4d-c550-455c-9651-d12a0671dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [None]*10\n",
    "for img in range(10):\n",
    "    probs[img] = raw_output_data[img].astype(float)\n",
    "    for i in range(10):        \n",
    "        probs[img][i] = (raw_output_data[img][i] + 128) / 255\n",
    "        print(\"{:8.6f}\".format(probs[img][i]),end=\" \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e03a5-dd08-4e00-8cac-f303f1ee2173",
   "metadata": {},
   "source": [
    "check if probabilities sum to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f2177-9943-43f8-b73c-288987ada0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in range(10):\n",
    "    sum = 0\n",
    "    for i in range(10):\n",
    "        sum += probabilities[img][i]\n",
    "    print(\"sum of probabilities for digit {:d}: {:8.2f}\".format(img,sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a6816-b52f-4290-a49c-fb329d024a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in range(10):\n",
    "    sum = 0\n",
    "    for i in range(10):\n",
    "        sum += probs[img][i]\n",
    "    print(\"sum of probabilities for digit {:d} for custom dequantization: {:8.2f}\".format(img,sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4a039-1cdf-4274-8243-9fe8de39c776",
   "metadata": {},
   "source": [
    "Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4b42f-dea3-4b4b-a3cd-a04f5e375c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as colors\n",
    "plt.matshow(probs,cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9d504-121f-47de-9e6e-2eddb040dcc1",
   "metadata": {},
   "source": [
    "The values are either vefy close to 1 or very small. We therefore only see the diagonal of the confusion matrix.  \n",
    "If we want to see the examples, where the probability to have found the right digit is less than 1, then we must use a log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cf94f-ef7a-4b32-8517-9f5a0eaf7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as colors\n",
    "plt.matshow(probs,norm=colors.LogNorm(0.001,1), cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ffc3-bd7d-4582-8482-918ec93f71b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
