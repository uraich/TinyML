{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d56c3f-9dea-43bc-ada1-1163fbfe0298",
   "metadata": {},
   "source": [
    "# Read the example wav file\n",
    "First we decode the file header, then we get at the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5ebab8-5544-450a-ac22-6b7d2af0f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_wav_header(audio_buffer):\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Show the wav header information\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    \n",
    "    riff = audio_buffer[:4].decode()\n",
    "    print(\"File type: {}\".format(riff))\n",
    "\n",
    "    file_size = audio_buffer[7] << 24 | audio_buffer[6] << 16 | audio_buffer[5] << 8 | audio_buffer[4] \n",
    "    print(\"File size: 0x{:d}\".format(file_size))\n",
    "\n",
    "    audio_type = audio_buffer[8:12].decode()\n",
    "    print(\"Audio type: {}\".format(audio_type))\n",
    "\n",
    "    format_marker = audio_buffer[12:15].decode()\n",
    "    print(\"Format marker: {}\".format(format_marker))\n",
    "\n",
    "    data_length = audio_buffer[17] << 8 | audio_buffer[16]\n",
    "    print(\"Data length in bits: {:d}\".format(data_length))\n",
    "\n",
    "    type_format = audio_buffer[21] << 8 | audio_buffer[20]\n",
    "    if type_format == 1:\n",
    "        print(\"PCM - 2 byte integer\")\n",
    "    else:\n",
    "        print(\"Unknown format: {d}\".format(type_format))\n",
    "\n",
    "    no_of_channels = audio_buffer[23] << 8 | audio_buffer[22]\n",
    "    print(\"No of channels: {:d}\".format(no_of_channels))\n",
    "\n",
    "    sample_rate = audio_buffer[27] << 24 | audio_buffer[26] << 16 | audio_buffer[25] << 8 | audio_buffer[24]\n",
    "    print(\"Sample rate: {:d} Hz\".format(sample_rate))\n",
    "\n",
    "    s_rate_bps_ch = audio_buffer[31] << 24 | audio_buffer[30] << 16 | audio_buffer[29] << 8 | audio_buffer[28]\n",
    "    print(\"(Sample rate * Bits per sample * Channels)/8: {:d}\".format(s_rate_bps_ch))\n",
    "\n",
    "    bts_ch = audio_buffer[35] << 8 | audio_buffer[34]\n",
    "    print(\"(Bits per sample * channels)/8: {:d}\".format(bts_ch))\n",
    "\n",
    "    bits_per_sample = audio_buffer[35] << 8 | audio_buffer[34]\n",
    "    print(\"Bits per sample: {:d}\".format(bits_per_sample))\n",
    "\n",
    "    data_section = audio_buffer[36:40].decode()\n",
    "    print(\"Start of data section: {}\".format(data_section))\n",
    "\n",
    "    data_section_length = audio_buffer[43] << 24 | audio_buffer[42] << 16 | audio_buffer[41] << 8 | audio_buffer[40]\n",
    "    print(\"Length of data section: {:d}\".format(data_section_length))\n",
    "    return sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70686ac6-68e8-4370-a775-3050ff0d043c",
   "metadata": {},
   "source": [
    "Read the wav file into a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063a70bf-55fa-4bb7-8eee-da5e71e8f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = \"wav_files/yes-example.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1658ea02-8f15-4122-9c9f-ad3c247ef4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(audiofile,'rb')\n",
    "except:\n",
    "    print(\"Cannot open file {}\".format(audiofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8c2f72-f705-460f-aa56-bf6c73d76b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_buffer = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03447da8-ce53-470c-bcfb-5dd243fb54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "Show the wav header information\n",
      "-------------------------------------------------------------------------\n",
      "File type: RIFF\n",
      "File size: 0x32036\n",
      "Audio type: WAVE\n",
      "Format marker: fmt\n",
      "Data length in bits: 16\n",
      "PCM - 2 byte integer\n",
      "No of channels: 1\n",
      "Sample rate: 16000 Hz\n",
      "(Sample rate * Bits per sample * Channels)/8: 32000\n",
      "(Bits per sample * channels)/8: 16\n",
      "Bits per sample: 16\n",
      "Start of data section: data\n",
      "Length of data section: 32000\n"
     ]
    }
   ],
   "source": [
    "kAudioSampleFrequency = decode_wav_header(audio_buffer)\n",
    "kAudioOneMsSize = kAudioSampleFrequency // 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ee459e-387f-4875-9343-7948d4acd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following values are derived from values used during model training.\n",
    "# If you change the way you preprocess the input, update all these constants.\n",
    "kFeatureSliceSize = 40\n",
    "kFeatureSliceCount = 49\n",
    "kFeatureElementCount = (kFeatureSliceSize * kFeatureSliceCount)\n",
    "kFeatureSliceStrideMs = 20\n",
    "kFeatureSliceDurationMs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008d242d-7d7e-4bf5-8e25-6dcfc8f659e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kSilenceIndex = 0\n",
    "kUnknownIndex = 1\n",
    "kYesIndex = 2\n",
    "kNoIndex = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f271629-7040-4059-a3fa-349df15480c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window size in samples: 480\n",
      "Stride size in samples: 320\n"
     ]
    }
   ],
   "source": [
    "stride_size = kFeatureSliceStrideMs * kAudioOneMsSize\n",
    "window_size = kFeatureSliceDurationMs * kAudioOneMsSize\n",
    "print(\"\\nWindow size in samples: {:d}\".format(window_size))\n",
    "print(\"Stride size in samples: {:d}\".format(stride_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f299bf9-49bd-4124-a320-8dc86c4340be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791a2d1-9d21-4cd2-94ac-8a90a9220467",
   "metadata": {},
   "source": [
    "Convert from bytearray to int16 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de957128-24fd-4eae-b874-61045e5d3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array = np.frombuffer(audio_buffer[44:], dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7603457-3c40-408e-be73-9220ce5174e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of audio samples: 16000\n"
     ]
    }
   ],
   "source": [
    "count = audio_array.size\n",
    "print(\"No of audio samples: {:d}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1279d28-c2ba-42a4-8241-f6ccb4f1d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_10ms = np.zeros(160,dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a41a76f-8186-408a-b178-c6d7064d6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureData:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.slices=[]\n",
    "        self.totalSlices = 0\n",
    "\n",
    "    def addSlice(self, slice):\n",
    "\n",
    "        self.totalSlices = self.totalSlices + 1\n",
    "        self.slices.append(slice)\n",
    "\n",
    "        if len (self.slices) > 49:\n",
    "            self.slices.pop(0)\n",
    "\n",
    "        print (\"total slices = %d\\n\" % self.totalSlices)\n",
    "        print (\"addSlice(): spectrogram length = %d\\n\" % spectrogram.size())\n",
    "        print (spectrogram)\n",
    "\n",
    "\n",
    "    def setInputTensorValues(self, inputTensor):\n",
    "        # print (inputTensor)\n",
    "        counter = 0\n",
    "        for slice_index in range(len(self.slices)):\n",
    "            slice = self.slices[slice_index]\n",
    "            spectrogram = slice.getSpectrogram()\n",
    "            for spectrogram_index in range (spectrogram.size()):\n",
    "                inputTensor.setValue(counter, spectrogram[spectrogram_index])\n",
    "                counter = counter + 1\n",
    "\n",
    "        # set 1960 values on input tensor\n",
    "        # print (\"set %d values on input tensor\\n\" % (counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99878519-2c40-4cff-9ff2-fe214ec438fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score:\n",
    "    def __init__(self, kind, score):\n",
    "        self.kind = kind\n",
    "        self.score = score\n",
    "        \n",
    "class Results:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.silence_data = []\n",
    "        self.unknown_data = []\n",
    "        self.yes_data = []\n",
    "        self.no_data  = []\n",
    "        self.index = 0\n",
    "\n",
    "    def _computeAverageTotal (self, array_data):\n",
    "        total = 0\n",
    "        array_length = len(array_data)\n",
    "        for i in range (array_length):\n",
    "            total = total + array_data[i]\n",
    "        return math.floor(total / array_length)\n",
    "\n",
    "    def computeResults(self):\n",
    "        topScore = 0\n",
    "        topScoreKind = None\n",
    "        silence = self._computeAverageTotal(self.silence_data)\n",
    "        print(\"Average total of silence: {:d}\".format(silence))\n",
    "\n",
    "        if silence > 200:\n",
    "            topScoreKind = \"silence\"\n",
    "            topScore = silence\n",
    "\n",
    "        unknown = self._computeAverageTotal(self.unknown_data)\n",
    "        print(\"Average total of unknown: {:d}\".format(unknown))\n",
    "\n",
    "        if unknown > topScore and unknown > 200:\n",
    "            topScoreKind = \"unknown\"\n",
    "            topScore = unknown\n",
    "        yes = self._computeAverageTotal(self.yes_data)\n",
    "        print(\"Average total of yes: {:d}\".format(yes))\n",
    "\n",
    "        if yes > topScore and yes > 200:\n",
    "            topScoreKind = \"yes\"\n",
    "            topScore = yes\n",
    "\n",
    "        no = self._computeAverageTotal(self.no_data)\n",
    "        print(\"Average total of no: {:d}\".format(no))\n",
    "\n",
    "        if no > topScore and no > 200:\n",
    "            topScoreKind = \"no\"\n",
    "            topScore = no\n",
    "\n",
    "        return Score (topScoreKind, topScore)\n",
    "\n",
    "    def storeResults(self, silenceScore, unknownScore, yesScore, noScore):\n",
    "        print(\"index: \",self.index)\n",
    "        if self.index == 3:\n",
    "            self.silence_data.pop(0)\n",
    "            self.unknown_data.pop(0)\n",
    "            self.yes_data.pop(0)\n",
    "            self.no_data.pop(0)\n",
    "        else:\n",
    "            self.index += 1\n",
    "\n",
    "        self.silence_data.append(silenceScore)\n",
    "        self.unknown_data.append(unknownScore)\n",
    "        self.yes_data.append(yesScore)\n",
    "        self.no_data.append(noScore)\n",
    "        print(\"Length of silence_data: \",len(self.silence_data), \n",
    "            \"last silence value: {:d}\".format(self.silence_data[len(self.silence_data)-1]))\n",
    "        print(\"Length of unknown_data: \",len(self.unknown_data),\n",
    "             \"last unknown value: {:d}\".format(self.unknown_data[len(self.unknown_data)-1]))            \n",
    "        print(\"Length of yes_data:     \",len(self.yes_data),\n",
    "             \"last yes value: {:d}\".format(self.yes_data[len(self.yes_data)-1]))                    \n",
    "        print(\"Length of no_data:      \",len(self.silence_data),\n",
    "              \"last no value: {:d}\".format(self.no_data[len(self.no_data)-1]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dcb2bd5-a519-457c-be6f-4cb1fac8863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "feature_data = FeatureData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1e9b14-e288-42aa-aaaa-f75f13f4d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6f57f22-66f4-439b-96f3-5183f1aca610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  0\n",
      "Length of silence_data:  1 last silence value: 0\n",
      "Length of unknown_data:  1 last unknown value: 0\n",
      "Length of yes_data:      1 last yes value: 201\n",
      "Length of no_data:       1 last no value: 0\n",
      "Average total of silence: 0\n",
      "Average total of unknown: 0\n",
      "Average total of yes: 201\n",
      "Average total of no: 0\n",
      "kind: yes, score: 201\n"
     ]
    }
   ],
   "source": [
    "r.storeResults(0, 0, 201, 0)\n",
    "score = r.computeResults()\n",
    "print(\"kind: {}, score: {}\".format(score.kind, score.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7416a2c7-ae96-45b4-b4b3-842ec4187bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  1\n",
      "Length of silence_data:  2 last silence value: 0\n",
      "Length of unknown_data:  2 last unknown value: 201\n",
      "Length of yes_data:      2 last yes value: 0\n",
      "Length of no_data:       2 last no value: 0\n",
      "Average total of silence: 0\n",
      "Average total of unknown: 100\n",
      "Average total of yes: 100\n",
      "Average total of no: 0\n",
      "kind: None, score: 0\n"
     ]
    }
   ],
   "source": [
    "r.storeResults(0, 201, 0, 0)\n",
    "score = r.computeResults()\n",
    "print(\"kind: {}, score: {}\".format(score.kind, score.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba7c3d-3da1-4f4b-86eb-8ec210be319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2b75a50-d6a0-466d-b053-d67f75bf211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentAudio(featureData, audio, trailing_10ms):\n",
    "    # In this example we have an array of 1 second of audio data.\n",
    "    # This is a 16,000 element array.\n",
    "    # the stride is how far over we adjust the start of the window on each step\n",
    "    # in this example it is 20 ms (20x16=320).\n",
    "    # The width of the window for which we capture the spectogram is 30ms (16x30=480).\n",
    "    # this function will turn the input array into a dictionary of start time to wav data\n",
    "\n",
    "    input_audio = np.concatenate((trailing_10ms, audio), axis=0)\n",
    "    input_size = input_audio.size\n",
    "\n",
    "    total_segments = math.floor(input_size / stride_size)\n",
    "    start_index = 0\n",
    "\n",
    "    for segment_index in range (total_segments):\n",
    "        end_index = min (start_index +  window_size, input_size)\n",
    "        # print (\"segment_index=%d,start_index=%d, end_index=%d, size=%d\\n\" % (segment_index, start_index, end_index, end_index-start_index))\n",
    "        slice = Slice (input_audio[start_index:end_index], start_index)\n",
    "        featureData.addSlice(slice)\n",
    "        start_index = start_index + stride_size\n",
    "\n",
    "    # return the trailing 10ms\n",
    "    return np.array(input_audio[input_size-160:input_size], dtype=np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd92dd29-e668-4fc2-8004-44b04b72b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Slice:\n",
    "\n",
    "    def __init__(self, segment, start_index):\n",
    "        # self.segment = segment\n",
    "        if segment.size != 480:\n",
    "            raise ValueError (\"Expected segment to be 480 bytes, was %d.\" % (segment.size()))\n",
    "\n",
    "        self.spectrogram = None\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def getSpectrogram(self):\n",
    "        return self.spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a3319ef-f198-44d3-bca5-987da938a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBufferSize=320*4\n",
    "while count > 0:\n",
    "    # segment the 16000 element array into 320*4 parts\n",
    "    currentStartIndex = start_index\n",
    "    currentEndIndex = currentStartIndex + inputBufferSize\n",
    "\n",
    "    currentSamples = np.array(audio_array[currentStartIndex:currentEndIndex], dtype=np.int16)\n",
    "\n",
    "    trailing_10ms = segmentAudio(featureData, currentSamples, trailing_10ms)\n",
    "\n",
    "    start_index = currentEndIndex\n",
    "    count = count - inputBufferSize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c35c18b8-d334-4630-bb82-174a956d21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of slices: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of slices: {:d}\".format(len(feature_data.slices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb1245-ca06-470e-afa1-e7855337c8af",
   "metadata": {},
   "source": [
    "Get the micro-speech quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0e5eb-a8f6-4889-a82a-ee29182df6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0942d71-61f6-4925-9737-0f72ea96fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tflite.Interpreter(model_path=\"models/number_model_quant.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
